{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os.path, datetime\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ALPHA_KEY = os.environ['ALPHA_KEY']\n",
    "ALPHA_KEY = 'B53N03ODVZVOH8R3'\n",
    "ts = TimeSeries(key=ALPHA_KEY,output_format='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(abbr):\n",
    "    data, metadata=ts.get_daily(abbr,outputsize='full')\n",
    "    data = data.iloc[::-1]\n",
    "    data['date'] = data.index\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadScale(data):\n",
    "    training_complete = data\n",
    "    training_processed = training_complete.iloc[:, 1:2].values\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    training_scaled = scaler.fit_transform(training_processed)\n",
    "    return training_complete, training_processed, scaler, training_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(training_scaled, abbr):\n",
    "    features_set = []\n",
    "    labels = []\n",
    "    end = len(training_scaled)-1\n",
    "    for i in range(60, end):\n",
    "        features_set.append(training_scaled[i-60:i, 0])\n",
    "        labels.append(training_scaled[i, 0])\n",
    "\n",
    "    # Convert both the feature_set and the labels list to the numpy array before we can use it for training\n",
    "    features_set, labels = np.array(features_set), np.array(labels)\n",
    "\n",
    "    # In order to train LSTM on our data, we need to convert our data into the shape accepted by the LSTM.\n",
    "    # We need to convert our data into three-dimensional format\n",
    "    features_set = np.reshape(features_set, (features_set.shape[0], features_set.shape[1], 1))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(features_set.shape[1], 1)))\n",
    "\n",
    "    # Add dropout layer to avoid overfitting of the data\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # To make our model more robust we add a dense layer\n",
    "    model.add(Dense(units = 1))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "    print(abbr)\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "    model.fit(features_set, labels, epochs = 100, batch_size = 32)\n",
    "\n",
    "    # Save Model\n",
    "    modelTarget = str('models/' + abbr + 'Model.h5')\n",
    "    model.save(modelTarget)\n",
    "    \n",
    "    # Pull in recent stock data to test the prediction model against - last 100 days of data\n",
    "    data2, metadata=ts.get_daily(abbr,outputsize='compact')\n",
    "    data2 = data2.iloc[::-1]\n",
    "\n",
    "    return model, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data2, scaler, abbr, model, training_complete):\n",
    "    testing_complete = data2\n",
    "    testing_processed = testing_complete.iloc[:, 1:2].values\n",
    "    total = pd.concat((training_complete['1. open'], testing_complete['1. open']), axis=0)\n",
    "    test_inputs = total[len(total) - len(testing_complete) - 60:].values\n",
    "    test_inputs = test_inputs.reshape(-1,1)\n",
    "    testii = test_inputs\n",
    "    test_inputs = scaler.transform(test_inputs)\n",
    "    testy = scaler.inverse_transform(test_inputs)\n",
    "    test_features = []\n",
    "    \n",
    "    for i in range(60, 161):\n",
    "        test_features.append(test_inputs[i-60:i, 0])\n",
    "    test_features = np.array(test_features)\n",
    "    test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], 1))\n",
    "    predictions = model.predict(test_features)\n",
    "\n",
    "    for i in range(0,10):\n",
    "        new_pred = [predictions[-1]]\n",
    "        test_inputs = np.append(test_inputs,new_pred)\n",
    "        test_features = []\n",
    "        for j in range(60, 161+i):\n",
    "            test_features.append(test_inputs[j-60:j])\n",
    "        test_features = np.array(test_features)\n",
    "        test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], 1))\n",
    "        predictions = model.predict(test_features)\n",
    "\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    print(predictions.shape)\n",
    "    print(test_inputs.shape)\n",
    "    print(predictions[-10::])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(abbr):\n",
    "    data = getData(abbr)\n",
    "    training_complete, training_processed, scaler, training_scaled = loadScale(data)\n",
    "    model, data2 = trainModel(training_scaled, abbr)\n",
    "    predict(data2, scaler, abbr, model, training_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAC\n",
      "BAC\n",
      "Epoch 1/100\n",
      "164/164 [==============================] - 10s 58ms/step - loss: 0.0075\n",
      "Epoch 2/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0030\n",
      "Epoch 3/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0027\n",
      "Epoch 4/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0024\n",
      "Epoch 5/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0022\n",
      "Epoch 6/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 0.0021\n",
      "Epoch 7/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0018\n",
      "Epoch 8/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 0.0018\n",
      "Epoch 9/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0017\n",
      "Epoch 10/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0015\n",
      "Epoch 11/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0015\n",
      "Epoch 12/100\n",
      "164/164 [==============================] - 9s 58ms/step - loss: 0.0013\n",
      "Epoch 13/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0014\n",
      "Epoch 14/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0012\n",
      "Epoch 15/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0012\n",
      "Epoch 16/100\n",
      "164/164 [==============================] - 9s 58ms/step - loss: 0.0012\n",
      "Epoch 17/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 0.0013\n",
      "Epoch 18/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 0.0011\n",
      "Epoch 19/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0011\n",
      "Epoch 20/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0010\n",
      "Epoch 21/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 9.1196e-04\n",
      "Epoch 22/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 0.0011\n",
      "Epoch 23/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0010\n",
      "Epoch 24/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 9.6454e-04\n",
      "Epoch 25/100\n",
      "164/164 [==============================] - 10s 58ms/step - loss: 9.2255e-04\n",
      "Epoch 26/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 9.8837e-04\n",
      "Epoch 27/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 9.6280e-04\n",
      "Epoch 28/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 9.4713e-04\n",
      "Epoch 29/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 8.9742e-04\n",
      "Epoch 30/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 8.6636e-04\n",
      "Epoch 31/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 9.1970e-04\n",
      "Epoch 32/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 8.4627e-04\n",
      "Epoch 33/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 8.3815e-04\n",
      "Epoch 34/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 8.1771e-04\n",
      "Epoch 35/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 8.2904e-04\n",
      "Epoch 36/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 9.1268e-04\n",
      "Epoch 37/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 8.6836e-04\n",
      "Epoch 38/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 8.3039e-04\n",
      "Epoch 39/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 7.8923e-04\n",
      "Epoch 40/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 8.3322e-04\n",
      "Epoch 41/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 8.2498e-04\n",
      "Epoch 42/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 7.8715e-04\n",
      "Epoch 43/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 8.3680e-04\n",
      "Epoch 44/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 8.1562e-04\n",
      "Epoch 45/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.5697e-04\n",
      "Epoch 46/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 8.6049e-04\n",
      "Epoch 47/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.4513e-04\n",
      "Epoch 48/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.7714e-04\n",
      "Epoch 49/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 8.2037e-04\n",
      "Epoch 50/100\n",
      "164/164 [==============================] - 10s 59ms/step - loss: 8.4926e-04\n",
      "Epoch 51/100\n",
      "164/164 [==============================] - 9s 58ms/step - loss: 7.3971e-04\n",
      "Epoch 52/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 8.0989e-04\n",
      "Epoch 53/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.9005e-04\n",
      "Epoch 54/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 7.6074e-04\n",
      "Epoch 55/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 7.8634e-04\n",
      "Epoch 56/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.0678e-04\n",
      "Epoch 57/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.7208e-04\n",
      "Epoch 58/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.3265e-04\n",
      "Epoch 59/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.1191e-04\n",
      "Epoch 60/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.8909e-04\n",
      "Epoch 61/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 7.1498e-04\n",
      "Epoch 62/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.7160e-04\n",
      "Epoch 63/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.9797e-04\n",
      "Epoch 64/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.9502e-04\n",
      "Epoch 65/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.8331e-04\n",
      "Epoch 66/100\n",
      "164/164 [==============================] - 9s 58ms/step - loss: 7.2880e-04\n",
      "Epoch 67/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 7.3156e-04\n",
      "Epoch 68/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.7138e-04\n",
      "Epoch 69/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 7.0807e-04\n",
      "Epoch 70/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.6784e-04\n",
      "Epoch 71/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 7.1725e-04\n",
      "Epoch 72/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.7223e-04\n",
      "Epoch 73/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.5150e-04\n",
      "Epoch 74/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.5829e-04\n",
      "Epoch 75/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.2518e-04\n",
      "Epoch 76/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.2629e-04\n",
      "Epoch 77/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.7281e-04\n",
      "Epoch 78/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.7625e-04\n",
      "Epoch 79/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.8726e-04\n",
      "Epoch 80/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.6407e-04\n",
      "Epoch 81/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.5128e-04\n",
      "Epoch 82/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.1807e-04\n",
      "Epoch 83/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.6280e-04\n",
      "Epoch 84/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.6591e-04\n",
      "Epoch 85/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.4814e-04\n",
      "Epoch 86/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 5.9357e-04\n",
      "Epoch 87/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.0703e-04\n",
      "Epoch 88/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.5598e-04\n",
      "Epoch 89/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.3442e-04\n",
      "Epoch 90/100\n",
      "164/164 [==============================] - 10s 58ms/step - loss: 6.1293e-04\n",
      "Epoch 91/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.8870e-04\n",
      "Epoch 92/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.4702e-04\n",
      "Epoch 93/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.4125e-04\n",
      "Epoch 94/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.7115e-04\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 9s 57ms/step - loss: 6.1206e-04\n",
      "Epoch 96/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.3965e-04\n",
      "Epoch 97/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.3527e-04\n",
      "Epoch 98/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 5.9588e-04\n",
      "Epoch 99/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.5533e-04\n",
      "Epoch 100/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.2402e-04\n",
      "(110, 1)\n",
      "(170,)\n",
      "[[23.610712]\n",
      " [23.4853  ]\n",
      " [23.630636]\n",
      " [23.632786]\n",
      " [23.686945]\n",
      " [23.718397]\n",
      " [23.763065]\n",
      " [23.799427]\n",
      " [23.833902]\n",
      " [23.861063]]\n",
      "T\n",
      "T\n",
      "Epoch 1/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0062\n",
      "Epoch 2/100\n",
      "164/164 [==============================] - 10s 58ms/step - loss: 0.0029\n",
      "Epoch 3/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0023\n",
      "Epoch 4/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0020\n",
      "Epoch 5/100\n",
      "164/164 [==============================] - 9s 58ms/step - loss: 0.0019\n",
      "Epoch 6/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 0.0019\n",
      "Epoch 7/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0015\n",
      "Epoch 8/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0015\n",
      "Epoch 9/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0014\n",
      "Epoch 10/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0013\n",
      "Epoch 11/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0012\n",
      "Epoch 12/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0012\n",
      "Epoch 13/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0011\n",
      "Epoch 14/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 0.0011\n",
      "Epoch 15/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 9.9811e-04\n",
      "Epoch 16/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 9.8604e-04\n",
      "Epoch 17/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 8.5676e-04\n",
      "Epoch 18/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 8.5409e-04\n",
      "Epoch 19/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 8.3267e-04\n",
      "Epoch 20/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 8.1126e-04\n",
      "Epoch 21/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.6361e-04\n",
      "Epoch 22/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.3501e-04\n",
      "Epoch 23/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.4405e-04\n",
      "Epoch 24/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 7.1900e-04\n",
      "Epoch 25/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 7.1618e-04\n",
      "Epoch 26/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 7.3380e-04\n",
      "Epoch 27/100\n",
      "164/164 [==============================] - 9s 58ms/step - loss: 6.9371e-04\n",
      "Epoch 28/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 6.8065e-04\n",
      "Epoch 29/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.4485e-04\n",
      "Epoch 30/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.9722e-04\n",
      "Epoch 31/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.5714e-04\n",
      "Epoch 32/100\n",
      "164/164 [==============================] - 10s 59ms/step - loss: 5.6039e-04\n",
      "Epoch 33/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.3456e-04\n",
      "Epoch 34/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 7.2436e-04\n",
      "Epoch 35/100\n",
      "164/164 [==============================] - 9s 58ms/step - loss: 6.2730e-04\n",
      "Epoch 36/100\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 6.5303e-04\n",
      "Epoch 37/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 6.5305e-04\n",
      "Epoch 38/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.9075e-04\n",
      "Epoch 39/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.0175e-04\n",
      "Epoch 40/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.3349e-04\n",
      "Epoch 41/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.8383e-04\n",
      "Epoch 42/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.8044e-04\n",
      "Epoch 43/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.5560e-04\n",
      "Epoch 44/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.9355e-04\n",
      "Epoch 45/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.9469e-04\n",
      "Epoch 46/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.1149e-04\n",
      "Epoch 47/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.1748e-04\n",
      "Epoch 48/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.6387e-04\n",
      "Epoch 49/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.9771e-04\n",
      "Epoch 50/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.6016e-04\n",
      "Epoch 51/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.4559e-04\n",
      "Epoch 52/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 5.2324e-04\n",
      "Epoch 53/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.4902e-04\n",
      "Epoch 54/100\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 5.8601e-04\n",
      "Epoch 55/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.5234e-04\n",
      "Epoch 56/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 6.0309e-04\n",
      "Epoch 57/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.1278e-04\n",
      "Epoch 58/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.8714e-04\n",
      "Epoch 59/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.6635e-04\n",
      "Epoch 60/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.7903e-04\n",
      "Epoch 61/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.7163e-04\n",
      "Epoch 62/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.6160e-04\n",
      "Epoch 63/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.2764e-04\n",
      "Epoch 64/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.4075e-04\n",
      "Epoch 65/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.2146e-04\n",
      "Epoch 66/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 5.6606e-04\n",
      "Epoch 67/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.6584e-04\n",
      "Epoch 68/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.5489e-04\n",
      "Epoch 69/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.4339e-04\n",
      "Epoch 70/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.4430e-04\n",
      "Epoch 71/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 5.4906e-04\n",
      "Epoch 72/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.4646e-04\n",
      "Epoch 73/100\n",
      "164/164 [==============================] - 10s 58ms/step - loss: 5.3805e-04\n",
      "Epoch 74/100\n",
      "164/164 [==============================] - 9s 56ms/step - loss: 5.3191e-04\n",
      "Epoch 75/100\n",
      "164/164 [==============================] - 10s 60ms/step - loss: 5.5853e-04\n",
      "Epoch 76/100\n",
      "164/164 [==============================] - 11s 67ms/step - loss: 5.2896e-04\n",
      "Epoch 77/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 5.4388e-04\n",
      "Epoch 78/100\n",
      "164/164 [==============================] - 10s 63ms/step - loss: 5.0679e-04\n",
      "Epoch 79/100\n",
      "164/164 [==============================] - 9s 55ms/step - loss: 5.3107e-04\n",
      "Epoch 80/100\n",
      "164/164 [==============================] - 9s 57ms/step - loss: 4.9573e-04\n",
      "Epoch 81/100\n",
      " 97/164 [================>.............] - ETA: 4s - loss: 4.8785e-04"
     ]
    }
   ],
   "source": [
    "# comps = [\"MSFT\",\"AAPL\",\"AMZN\",\"FB\",\"BRK-B\",\"GOOGL\",\"JNJ\",\"JPM\",\"V\",\"PG\",\"MA\",\"INTC\",\"UNH\",\"BAC\",\"T\"]\n",
    "comps = [\"BAC\",\"T\"]\n",
    "for entry in comps:\n",
    "    print(entry)\n",
    "    low = entry.lower()\n",
    "    modelName = str('models/' + low + 'Model.h5')\n",
    "    runModel(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
